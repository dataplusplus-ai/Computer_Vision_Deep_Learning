{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "densenet_vgg16_19ensemble.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dataplusplus-ai/Computer_Vision_Deep_Learning/blob/main/densenet_vgg16_19ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDRm4X7UnLa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "db064ae0-101c-47c0-f0bf-8e9cf3ac2589"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "!unzip -q \"/content/gdrive/My Drive/trainjpg\"\n",
        "!unzip -q \"/content/gdrive/My Drive/testjpg\"\n",
        "!unzip -q \"/content/gdrive/My Drive/validationjpg\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCQVHsV5tfy5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "dcc867db-f06e-4c5a-8af7-5612443901e2"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "sm = SMOTE(random_state=42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.25"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bvsuw9BoNazU"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOkTi3irtfvi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "25d649d0-2614-40c6-f9cc-8db89942fb7d"
      },
      "source": [
        "X_smote, y_smote = sm.fit_resample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kRZI-3zimhkx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "47ec7275-aa90-492b-93b2-e980f3c5152d"
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import json\n",
        "\n",
        "from functools import partial\n",
        "from pathlib import Path \n",
        "from tqdm import tqdm\n",
        "\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Conv2D, BatchNormalization, Activation\n",
        "from keras.layers import MaxPool2D , Flatten\n",
        "from keras.layers import AveragePooling2D, Input, Flatten\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
        "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.regularizers import l2\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "\n",
        "import os\n",
        "from keras.layers.core import Dropout\n",
        "from keras.layers.core import Flatten\n",
        "from keras.layers.core import Dense\n",
        "from keras.layers import Input\n",
        "from keras.models import Model\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD\n",
        "from keras.utils import plot_model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XrwJrcFst43v"
      },
      "source": [
        "# 25 % of healthy_wheat 142 images are 36 so val healthy_wheat 36\n",
        "# 25 % of leaf_rust 357 images are 90\n",
        "# 25 % of stem_rust 376 images are 94"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSdfkSyp1DWO"
      },
      "source": [
        "from PIL import Image\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-QDEDXJ1DQ1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrkx4NBwcTlZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "3686141f-1174-4923-ee70-1aea9f7feb1d"
      },
      "source": [
        "#print('no of test images is {}'.format(len(os.listdir('/content/testjpg'))))\n",
        "print('no of train images is {}+{}+{}'.format(len(os.listdir('/content/trainjpg/healthy_wheat')),\n",
        "                                               len(os.listdir('/content/trainjpg/stem_rust')),\n",
        "                                              len(os.listdir('/content/trainjpg/leaf_rust'))))\n",
        "print('no of validation images is {}+{}+{}'.format(len(os.listdir('/content/validationjpg/healthy_wheat')),\n",
        "                                               len(os.listdir('/content/validationjpg/stem_rust')),\n",
        "                                              len(os.listdir('/content/validationjpg/leaf_rust'))))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "no of train images is 106+282+267\n",
            "no of validation images is 36+94+90\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uTMVWyn-1DM2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrzcm4onOtjR"
      },
      "source": [
        "# from imgaug import augmenters as iaa\n",
        "\n",
        "# aug1 = iaa.CoarseDropout(p=0.10, size_percent=0.05)\n",
        "\n",
        "# def additional_augmenation(image):\n",
        "#     image = aug1.augment_image(image)\n",
        "\n",
        "#     return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2_SPu-H1WP4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Sl_8uHrjHO0"
      },
      "source": [
        "def get_random_eraser(p=0.5, s_l=0.02, s_h=0.4, r_1=0.3, r_2=1/0.3, v_l=0, v_h=255, pixel_level=False):\n",
        "\n",
        "  \"\"\"\n",
        "  \n",
        "  p : the probability that random erasing is performed\n",
        "  s_l, s_h : minimum / maximum proportion of erased area against input image\n",
        "  r_1, r_2 : minimum / maximum aspect ratio of erased area\n",
        "  v_l, v_h : minimum / maximum value for erased area\n",
        "  pixel_level : pixel-level randomization for erased area\n",
        "  \"\"\"\n",
        "\n",
        "  def eraser(input_img):\n",
        "    img_h, img_w, img_c = input_img.shape\n",
        "    p_1 = np.random.rand()\n",
        "\n",
        "    if p_1 > p:\n",
        "        return input_img\n",
        "\n",
        "    while True:\n",
        "        s = np.random.uniform(s_l, s_h) * img_h * img_w\n",
        "        r = np.random.uniform(r_1, r_2)\n",
        "        w = int(np.sqrt(s / r))\n",
        "        h = int(np.sqrt(s * r))\n",
        "        left = np.random.randint(0, img_w)\n",
        "        top = np.random.randint(0, img_h)\n",
        "\n",
        "        if left + w <= img_w and top + h <= img_h:\n",
        "            break\n",
        "\n",
        "    if pixel_level:\n",
        "        c = np.random.uniform(v_l, v_h, (h, w, img_c))\n",
        "    else:\n",
        "        c = np.random.uniform(v_l, v_h)\n",
        "\n",
        "    input_img[top:top + h, left:left + w, :] = c\n",
        "\n",
        "    return input_img\n",
        "\n",
        "  return eraser"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WW4Q3XQ8WZuM"
      },
      "source": [
        "img_width, img_height = 224,224\n",
        "batch_size=32\n",
        "train_datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        \n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=2,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.08,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.05,\n",
        "        # set range for random shear\n",
        "        shear_range=0.07,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.05,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.07,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.1,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        rescale=1. / 255,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        # set function that will be applied on each input\n",
        "        #Cutout Implementation\n",
        "        preprocessing_function=get_random_eraser(p=0.08,v_l=0, v_h=0.3, s_l=0.008, s_h=0.05, pixel_level=False),\n",
        "        #preprocessing_function=additional_augmenation, \n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXPba-DL1r1j"
      },
      "source": [
        "img_width, img_height = 224,224\n",
        "batch_size=32\n",
        "validation_datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        \n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=30,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.08,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.05,\n",
        "        # set range for random shear\n",
        "        shear_range=0.07,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.05,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.07,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.1,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        rescale=1. / 255,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        # set function that will be applied on each input\n",
        "        #Cutout Implementation\n",
        "        preprocessing_function=get_random_eraser(p=0.08,v_l=0, v_h=0.3, s_l=0.008, s_h=0.05, pixel_level=False),\n",
        "        #preprocessing_function=additional_augmenation, \n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_l_jM8FkHNn"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfaXfnEd1xkx"
      },
      "source": [
        "img_width, img_height = 224,224\n",
        "batch_size=32\n",
        "test_datagen = ImageDataGenerator(\n",
        "        # set input mean to 0 over the dataset\n",
        "        \n",
        "        # apply ZCA whitening\n",
        "        zca_whitening=False,\n",
        "        # epsilon for ZCA whitening\n",
        "        zca_epsilon=1e-06,\n",
        "        # randomly rotate images in the range (deg 0 to 180)\n",
        "        rotation_range=30,\n",
        "        # randomly shift images horizontally\n",
        "        width_shift_range=0.08,\n",
        "        # randomly shift images vertically\n",
        "        height_shift_range=0.05,\n",
        "        # set range for random shear\n",
        "        shear_range=0.07,\n",
        "        # set range for random zoom\n",
        "        zoom_range=0.05,\n",
        "        # set range for random channel shifts\n",
        "        channel_shift_range=0.07,\n",
        "        # set mode for filling points outside the input boundaries\n",
        "        fill_mode='nearest',\n",
        "        # value used for fill_mode = \"constant\"\n",
        "        cval=0.1,\n",
        "        # randomly flip images\n",
        "        horizontal_flip=True,\n",
        "        # randomly flip images\n",
        "        vertical_flip=False,\n",
        "        rescale=1. / 255,\n",
        "        # set rescaling factor (applied before any other transformation)\n",
        "        # set function that will be applied on each input\n",
        "        #Cutout Implementation\n",
        "        preprocessing_function=get_random_eraser(p=0.08,v_l=0, v_h=0.3, s_l=0.008, s_h=0.05, pixel_level=False),\n",
        "        #preprocessing_function=additional_augmenation, \n",
        "        # image data format, either \"channels_first\" or \"channels_last\"\n",
        "        data_format=None,\n",
        "        # fraction of images reserved for validation (strictly between 0 and 1)\n",
        "        validation_split=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3eSz-ZGey0F",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "56a158eb-fcc1-4666-f4cb-b1e49f24cbf7"
      },
      "source": [
        "#behaves weird, create a folder in content, move trainjpg there. Then create trainjpg here in content adn move from there\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    directory=r\"/content/trainjpg/\",\n",
        "    target_size=(224,224),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=True,\n",
        "    seed=42\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 655 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTEpGse91wx7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "880ef478-179d-41f7-dbe8-b0c54b72b8b3"
      },
      "source": [
        "#validation_datagen = ImageDataGenerator(rescale=1. / 255)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 220 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPLWrTA5Om7s",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "569d6646-ceb6-431a-abf9-ac1794c51274"
      },
      "source": [
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "    directory=r\"/content/validationjpg/\",\n",
        "    target_size=(224,224),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    seed=42,\n",
        "    class_mode='categorical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 220 images belonging to 3 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMTaEtyQkuka",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8270a3ba-b6c3-4891-8f4e-110b99185099"
      },
      "source": [
        "test_generator = test_datagen.flow_from_directory(\n",
        "    directory=r\"/content/ttest/\",\n",
        "    target_size=(224,224),\n",
        "    color_mode=\"rgb\",\n",
        "    batch_size=32,\n",
        "    class_mode=\"categorical\",\n",
        "    shuffle=False,\n",
        "    seed=42\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 610 images belonging to 1 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQt0sP5uS-2v"
      },
      "source": [
        "def lr_schedule(epoch):\n",
        "\n",
        "    lr = 1e-3\n",
        "    if epoch > 90:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 60:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 40:\n",
        "        lr *= 1e-1\n",
        "\n",
        "    print('Learning Rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HZJn88sZb2Vw"
      },
      "source": [
        "\n",
        "from keras.callbacks import Callback\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "# Keras-Contib Implementation\n",
        "class CyclicLR(Callback):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            base_lr=0.001,\n",
        "            max_lr=0.005,\n",
        "            step_size=2000.,\n",
        "            mode='triangular',\n",
        "            gamma=1.,\n",
        "            scale_fn=None,\n",
        "            scale_mode='cycle'):\n",
        "        super(CyclicLR, self).__init__()\n",
        "\n",
        "        if mode not in ['triangular', 'triangular2',\n",
        "                        'exp_range']:\n",
        "            raise KeyError(\"mode must be one of 'triangular', \"\n",
        "                           \"'triangular2', or 'exp_range'\")\n",
        "        self.base_lr = base_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.step_size = step_size\n",
        "        self.mode = mode\n",
        "        self.gamma = gamma\n",
        "        if scale_fn is None:\n",
        "            if self.mode == 'triangular':\n",
        "                self.scale_fn = lambda x: 1.\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'triangular2':\n",
        "                self.scale_fn = lambda x: 1 / (2.**(x - 1))\n",
        "                self.scale_mode = 'cycle'\n",
        "            elif self.mode == 'exp_range':\n",
        "                self.scale_fn = lambda x: gamma ** x\n",
        "                self.scale_mode = 'iterations'\n",
        "        else:\n",
        "            self.scale_fn = scale_fn\n",
        "            self.scale_mode = scale_mode\n",
        "        self.clr_iterations = 0.\n",
        "        self.trn_iterations = 0.\n",
        "        self.history = {}\n",
        "\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self, new_base_lr=None, new_max_lr=None,\n",
        "               new_step_size=None):\n",
        "        \"\"\"Resets cycle iterations.\n",
        "        Optional boundary/step size adjustment.\n",
        "        \"\"\"\n",
        "        if new_base_lr is not None:\n",
        "            self.base_lr = new_base_lr\n",
        "        if new_max_lr is not None:\n",
        "            self.max_lr = new_max_lr\n",
        "        if new_step_size is not None:\n",
        "            self.step_size = new_step_size\n",
        "        self.clr_iterations = 0.\n",
        "\n",
        "    def clr(self):\n",
        "        cycle = np.floor(1 + self.clr_iterations / (2 * self.step_size))\n",
        "        x = np.abs(self.clr_iterations / self.step_size - 2 * cycle + 1)\n",
        "        if self.scale_mode == 'cycle':\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
        "                np.maximum(0, (1 - x)) * self.scale_fn(cycle)\n",
        "        else:\n",
        "            return self.base_lr + (self.max_lr - self.base_lr) * \\\n",
        "                np.maximum(0, (1 - x)) * self.scale_fn(self.clr_iterations)\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        logs = logs or {}\n",
        "\n",
        "        if self.clr_iterations == 0:\n",
        "            K.set_value(self.model.optimizer.lr, self.base_lr)\n",
        "        else:\n",
        "            K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "    def on_batch_end(self, epoch, logs=None):\n",
        "\n",
        "        logs = logs or {}\n",
        "        self.trn_iterations += 1\n",
        "        self.clr_iterations += 1\n",
        "        K.set_value(self.model.optimizer.lr, self.clr())\n",
        "\n",
        "        self.history.setdefault(\n",
        "            'lr', []).append(\n",
        "            K.get_value(\n",
        "                self.model.optimizer.lr))\n",
        "        self.history.setdefault('iterations', []).append(self.trn_iterations)\n",
        "\n",
        "        for k, v in logs.items():\n",
        "            self.history.setdefault(k, []).append(v)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        print(\"Learning Rate: \", float(K.get_value(self.model.optimizer.lr)))\n",
        "        logs = logs or {}\n",
        "        logs['lr'] = K.get_value(self.model.optimizer.lr)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfr0a_XzDz4y"
      },
      "source": [
        "class SaveWeights(keras.callbacks.Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.i = 0 \n",
        "        self.ep = 0  \n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        \n",
        "        self.i += 1\n",
        "        self.ep = epoch + 1\n",
        "        #print(\"Self I: \", self.i, \" Epoch: \", self.ep)\n",
        "        if (self.i % 10) == 0:\n",
        "              densenetmodel.save_weights(f\"/content/gdrive/My Drive/densenet__{self.ep}.hdf5\")\n",
        "              print(\"Saved the Model after Epoch: \", self.i)\n",
        "        \n",
        "save_weight = SaveWeights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Lf9vlVJGD1R"
      },
      "source": [
        "\n",
        "\n",
        "# Prepare callbacks for model saving and for learning rate adjustment.\n",
        "\n",
        "\n",
        "filepath='/content/gdrive/My Drive/densenet:{epoch:03d}-validation.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath=filepath,\n",
        "                             verbose=1,\n",
        "                             save_best_only=True)\n",
        "\n",
        "clr = CyclicLR(base_lr=0.001, \n",
        "               max_lr=0.006, \n",
        "               step_size=2000., \n",
        "               mode='triangular')\n",
        "\n",
        "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1),\n",
        "                               cooldown=0,\n",
        "                               patience=5,\n",
        "                               min_lr=0.5e-6)\n",
        "\n",
        "callbacks = [checkpoint, lr_reducer, clr, save_weight]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-Y3g7ymD8vx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-V9jxuMf39g"
      },
      "source": [
        "from keras import models\n",
        "from keras import layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Odqn1nhc8myu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 546
        },
        "outputId": "70239ed8-1935-4cb2-9469-51126d0926db"
      },
      "source": [
        "from keras.applications.densenet import DenseNet121\n",
        "densenet_conv_base=DenseNet121(weights='imagenet',include_top=False,input_shape=(224,224,3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4271: The name tf.nn.avg_pool is deprecated. Please use tf.nn.avg_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/keras-team/keras-applications/releases/download/densenet/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "29089792/29084464 [==============================] - 3s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ck0bSBix9qxL"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D\n",
        "model=densenet_conv_base.output\n",
        "\n",
        "model=GlobalAveragePooling2D()(model)\n",
        "\n",
        "predictions = Dense(3, activation= 'softmax')(model)\n",
        "densenetmodel = Model(inputs = densenet_conv_base.input, outputs = predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fy0UOF4c-Qz2"
      },
      "source": [
        "densenet_conv_base.trainable=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lGcCng7t9raE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "3f5d333c-e935-4f52-ae2b-b83a93752e03"
      },
      "source": [
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "densenetmodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vOAXHqB-gOE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nMgMVJ75-pD9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5f3de506-3401-4132-af92-27e30a73adae"
      },
      "source": [
        "nb_epoch=150\n",
        "last_executed_epoch=0\n",
        "\n",
        "print('Training for Epochs : ' + str(last_executed_epoch+1) + ' - ' + str(nb_epoch) + \" -------------------------------\")\n",
        "\n",
        "# # Fit the model on the batches generated by datagen.flow().\n",
        "densenetmodel.fit_generator(generator=train_generator, validation_data=validation_generator, use_multiprocessing=True, epochs=nb_epoch, initial_epoch=last_executed_epoch, verbose=1, workers=6, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for Epochs : 1 - 150 -------------------------------\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/150\n",
            "21/21 [==============================] - 110s 5s/step - loss: 1.0345 - acc: 0.4847 - val_loss: 0.6104 - val_acc: 0.7500\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.61041, saving model to /content/gdrive/My Drive/densenet:001-validation.hdf5\n",
            "Learning Rate:  0.001052499981597066\n",
            "Epoch 2/150\n",
            "21/21 [==============================] - 28s 1s/step - loss: 0.5063 - acc: 0.7927 - val_loss: 0.6415 - val_acc: 0.7182\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.61041\n",
            "Learning Rate:  0.0011050000321120024\n",
            "Epoch 3/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.3032 - acc: 0.9078\n",
            "Epoch 00002: val_loss did not improve from 0.61041\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.2971 - acc: 0.9106 - val_loss: 0.5888 - val_acc: 0.7455\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.61041 to 0.58879, saving model to /content/gdrive/My Drive/densenet:003-validation.hdf5\n",
            "Learning Rate:  0.001157499966211617\n",
            "Epoch 4/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.2395 - acc: 0.9210\n",
            "21/21 [==============================] - 55s 3s/step - loss: 0.2405 - acc: 0.9220 - val_loss: 0.4128 - val_acc: 0.8636\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.58879 to 0.41281, saving model to /content/gdrive/My Drive/densenet:004-validation.hdf5\n",
            "Learning Rate:  0.0012100000167265534\n",
            "Epoch 5/150\n",
            "21/21 [==============================] - 55s 3s/step - loss: 0.1950 - acc: 0.9373 - val_loss: 0.4688 - val_acc: 0.8500\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.001262499950826168\n",
            "Epoch 6/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.1322 - acc: 0.9592Learning Rate:  0.001262499950826168\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.1301 - acc: 0.9597 - val_loss: 0.4517 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0013150000013411045\n",
            "Epoch 7/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.1110 - acc: 0.9642 - val_loss: 0.4699 - val_acc: 0.8273\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.001367500051856041\n",
            "Epoch 8/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0879 - acc: 0.9781Learning Rate:  0.001367500051856041\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0894 - acc: 0.9777 - val_loss: 0.4572 - val_acc: 0.8682\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0014199999859556556\n",
            "Epoch 9/150\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0673 - acc: 0.9850 - val_loss: 0.4518 - val_acc: 0.8636\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0004656453966163099\n",
            "Epoch 10/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0557 - acc: 0.9859\n",
            "Epoch 00009: val_loss did not improve from 0.41281\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0577 - acc: 0.9851 - val_loss: 0.5298 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0015249999705702066\n",
            "Saved the Model after Epoch:  10\n",
            "Epoch 11/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0448 - acc: 0.9906\n",
            "Epoch 00010: val_loss did not improve from 0.41281\n",
            "21/21 [==============================] - 57s 3s/step - loss: 0.0475 - acc: 0.9881 - val_loss: 0.5383 - val_acc: 0.8500\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.001577500021085143\n",
            "Epoch 12/150\n",
            "20/21 [===========================>..] - ETA: 3s - loss: 0.0536 - acc: 0.9891\n",
            "Epoch 00011: val_loss did not improve from 0.41281\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0546 - acc: 0.9881 - val_loss: 0.4950 - val_acc: 0.8636\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0016299999551847577\n",
            "Epoch 13/150\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0401 - acc: 0.9881 - val_loss: 0.5348 - val_acc: 0.8682\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0016825000056996942\n",
            "Epoch 14/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0429 - acc: 0.9896 - val_loss: 0.5067 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.000548655167222023\n",
            "Epoch 15/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0368 - acc: 0.9911 - val_loss: 0.4584 - val_acc: 0.8636\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0017874999903142452\n",
            "Epoch 16/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0467 - acc: 0.9875\n",
            "Epoch 00015: val_loss did not improve from 0.41281\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0450 - acc: 0.9881 - val_loss: 0.5010 - val_acc: 0.8409\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0018400000408291817\n",
            "Epoch 17/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0343 - acc: 0.9911 - val_loss: 0.5072 - val_acc: 0.8909\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0018924999749287963\n",
            "Epoch 18/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0276 - acc: 0.9940 - val_loss: 0.4441 - val_acc: 0.8955\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0019450000254437327\n",
            "Epoch 19/150\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0308 - acc: 0.9911 - val_loss: 0.4914 - val_acc: 0.8864\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0006316649378277361\n",
            "Epoch 20/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0436 - acc: 0.9896 - val_loss: 0.4951 - val_acc: 0.8682\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.002050000010058284\n",
            "Saved the Model after Epoch:  20\n",
            "Epoch 21/150\n",
            "21/21 [==============================] - 58s 3s/step - loss: 0.0460 - acc: 0.9836 - val_loss: 0.5383 - val_acc: 0.8727\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0021025000605732203\n",
            "Epoch 22/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0430 - acc: 0.9880 - val_loss: 0.4654 - val_acc: 0.8818\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0021550001110881567\n",
            "Epoch 23/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0376 - acc: 0.9896 - val_loss: 0.6333 - val_acc: 0.8818\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0022074999287724495\n",
            "Epoch 24/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0470 - acc: 0.9826\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0471 - acc: 0.9835 - val_loss: 0.7759 - val_acc: 0.8409\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0007146747666411102\n",
            "Epoch 25/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0315 - acc: 0.9895 - val_loss: 0.5799 - val_acc: 0.8864\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0023125000298023224\n",
            "Epoch 26/150\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0188 - acc: 0.9940 - val_loss: 0.5963 - val_acc: 0.8864\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.002365000080317259\n",
            "Epoch 27/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0166 - acc: 0.9953\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0163 - acc: 0.9955 - val_loss: 0.5684 - val_acc: 0.8818\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0024174998980015516\n",
            "Epoch 28/150\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0304 - acc: 0.9865 - val_loss: 0.5877 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.002469999948516488\n",
            "Epoch 29/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0376 - acc: 0.9906\n",
            "Epoch 00028: val_loss did not improve from 0.41281\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0387 - acc: 0.9896 - val_loss: 0.7360 - val_acc: 0.8409\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0007976845372468233\n",
            "Epoch 30/150\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0167 - acc: 0.9926 - val_loss: 0.6651 - val_acc: 0.8409\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.002575000049546361\n",
            "Saved the Model after Epoch:  30\n",
            "Epoch 31/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0113 - acc: 0.9969Learning Rate:  0.002575000049546361\n",
            "21/21 [==============================] - 58s 3s/step - loss: 0.0112 - acc: 0.9970 - val_loss: 0.5559 - val_acc: 0.8818\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0026275001000612974\n",
            "Epoch 32/150\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0232 - acc: 0.9911 - val_loss: 0.5189 - val_acc: 0.8773\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.00267999991774559\n",
            "Epoch 33/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0198 - acc: 0.9938\n",
            "Epoch 00032: val_loss did not improve from 0.41281\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0191 - acc: 0.9940 - val_loss: 0.6829 - val_acc: 0.8500\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0027324999682605267\n",
            "Epoch 34/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0247 - acc: 0.9940 - val_loss: 0.7266 - val_acc: 0.8455\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0008806943078525364\n",
            "Epoch 35/150\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0104 - acc: 0.9970 - val_loss: 0.8944 - val_acc: 0.8545\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0028375000692903996\n",
            "Epoch 36/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0197 - acc: 0.9922\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
            "Epoch 00035: val_loss did not improve from 0.41281\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0188 - acc: 0.9926 - val_loss: 0.6748 - val_acc: 0.8636\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0028899998869746923\n",
            "Epoch 37/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0138 - acc: 0.9969\n",
            "Epoch 00036: val_loss did not improve from 0.41281\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0134 - acc: 0.9970 - val_loss: 0.5946 - val_acc: 0.8773\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.002942499937489629\n",
            "Epoch 38/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0310 - acc: 0.9864 - val_loss: 0.5763 - val_acc: 0.8864\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0029949999880045652\n",
            "Epoch 39/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0179 - acc: 0.9955 - val_loss: 0.5007 - val_acc: 0.8864\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0009637041366659105\n",
            "Epoch 40/150\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0199 - acc: 0.9940 - val_loss: 0.5707 - val_acc: 0.9000\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.003100000089034438\n",
            "Saved the Model after Epoch:  40\n",
            "Epoch 41/150\n",
            "21/21 [==============================] - 58s 3s/step - loss: 0.0231 - acc: 0.9911 - val_loss: 0.6385 - val_acc: 0.8773\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.003152499906718731\n",
            "Epoch 42/150\n",
            "21/21 [==============================] - 63s 3s/step - loss: 0.0254 - acc: 0.9895 - val_loss: 0.7970 - val_acc: 0.8318\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0032049999572336674\n",
            "Epoch 43/150\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0671 - acc: 0.9775 - val_loss: 0.7317 - val_acc: 0.8500\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.003257500007748604\n",
            "Epoch 44/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0521 - acc: 0.9828\n",
            "Epoch 00043: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.003257500007748604\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0498 - acc: 0.9836 - val_loss: 0.6338 - val_acc: 0.8682\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0010467139072716236\n",
            "Epoch 45/150\n",
            "20/21 [===========================>..] - ETA: 2s - loss: 0.0234 - acc: 0.9891\n",
            "Epoch 00044: val_loss did not improve from 0.41281\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0231 - acc: 0.9896 - val_loss: 0.6134 - val_acc: 0.8909\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0033625001087784767\n",
            "Epoch 46/150\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0188 - acc: 0.9940 - val_loss: 0.6634 - val_acc: 0.8773\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.0034149999264627695\n",
            "Epoch 47/150\n",
            "21/21 [==============================] - 62s 3s/step - loss: 0.0687 - acc: 0.9713 - val_loss: 0.7108 - val_acc: 0.8409\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.41281\n",
            "Learning Rate:  0.003467499976977706\n",
            "Epoch 48/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-573:\n",
            "Process ForkPoolWorker-568:\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-575:\n",
            "Process ForkPoolWorker-572:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Process ForkPoolWorker-565:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Process ForkPoolWorker-576:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-566:\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-574:\n",
            "Process ForkPoolWorker-569:\n",
            "Process ForkPoolWorker-570:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-571:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Process ForkPoolWorker-567:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 238, in _get_batches_of_transformed_samples\n",
            "    x = self.image_data_generator.apply_transform(x, params)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 253, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 253, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 253, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/image_data_generator.py\", line 870, in apply_transform\n",
            "    order=self.interpolation_order)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 253, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 253, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in apply_affine_transform\n",
            "    cval=cval) for x_channel in x]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
            "    result = (True, func(*args, **kwds))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 253, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\", line 406, in get_index\n",
            "    return _SHARED_SEQUENCES[uid][i]\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 65, in __getitem__\n",
            "    return self._get_batches_of_transformed_samples(index_array)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 253, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/affine_transformations.py\", line 333, in <listcomp>\n",
            "    cval=cval) for x_channel in x]\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/iterator.py\", line 230, in _get_batches_of_transformed_samples\n",
            "    interpolation=self.interpolation)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 253, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 253, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/scipy/ndimage/interpolation.py\", line 486, in affine_transform\n",
            "    output, order, mode, cval, None, None)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/keras_preprocessing/image/utils.py\", line 132, in load_img\n",
            "    img = img.resize(width_height_tuple, resample)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/Image.py\", line 1886, in resize\n",
            "    self.load()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 209, in load\n",
            "    self.load_prepare()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 253, in load\n",
            "    n, err_code = decoder.decode(b)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/PIL/ImageFile.py\", line 279, in load_prepare\n",
            "    self.im = Image.core.new(self.mode, self.size)\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-811263c186b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# # Fit the model on the batches generated by datagen.flow().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdensenetmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_executed_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    604\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 638\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    639\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiIj5t0L-gDB"
      },
      "source": [
        "densenetmodel.load_weights('/content/gdrive/My Drive/densenet:004-validation.hdf5')\n",
        "#Learning Rate:  0.0012100000167265534')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ELR1sEp6LNfy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "8ff4b93c-295b-4ade-bdbe-6ac0ea092f11"
      },
      "source": [
        "scores=densenetmodel.evaluate_generator(validation_generator,verbose=1)\n",
        "scores"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "7/7 [==============================] - 17s 2s/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6272263906218789, 0.8590909058397467]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nzYIAdT6LOfE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2gMi3qch58U",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "462ff04f-e6a2-4565-dbd2-4667428447ea"
      },
      "source": [
        "from keras.applications.vgg19 import VGG19\n",
        "vgg19_conv_base=VGG19(weights='imagenet',include_top=False,input_shape=(224,224,3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "80142336/80134624 [==============================] - 7s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adSaRGwT8imf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JZTORGkPEOgi"
      },
      "source": [
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D,GlobalAveragePooling2D\n",
        "model=vgg19_conv_base.output\n",
        "\n",
        "model=GlobalAveragePooling2D()(model)\n",
        "\n",
        "predictions = Dense(3, activation= 'softmax')(model)\n",
        "vgg19_freezemodel = Model(inputs = vgg19_conv_base.input, outputs = predictions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R00qX2kEVgv"
      },
      "source": [
        "vgg19_conv_base.trainable=False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svZLBh9QEZG-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "9e1d604c-5fa8-4a17-e1c1-2899107eba35"
      },
      "source": [
        "opt = SGD(lr=0.001, momentum=0.9)\n",
        "vgg19_freezemodel.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opt,\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y09hpJ_oEf1o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cDRqnBemp_d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b01846ee-412b-4680-bbbf-f2abe7ee545e"
      },
      "source": [
        "nb_epoch=150\n",
        "last_executed_epoch=0\n",
        "\n",
        "print('Training for Epochs : ' + str(last_executed_epoch+1) + ' - ' + str(nb_epoch) + \" -------------------------------\")\n",
        "\n",
        "# # Fit the model on the batches generated by datagen.flow().\n",
        "vgg19_freezemodel.fit_generator(generator=train_generator, validation_data=validation_generator, use_multiprocessing=True, epochs=nb_epoch, initial_epoch=last_executed_epoch, verbose=1, workers=6, callbacks=callbacks)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training for Epochs : 1 - 150 -------------------------------\n",
            "Epoch 1/150\n",
            "21/21 [==============================] - 73s 3s/step - loss: 0.0582 - acc: 0.9806 - val_loss: 0.6097 - val_acc: 0.8364\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.60972, saving model to /content/gdrive/My Drive/vgg19_32:001-validation.hdf5\n",
            "Learning Rate:  0.001052499981597066\n",
            "Epoch 2/150\n",
            "21/21 [==============================] - 61s 3s/step - loss: 0.0260 - acc: 0.9940 - val_loss: 0.6825 - val_acc: 0.8136\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.60972\n",
            "Learning Rate:  0.0011050000321120024\n",
            "Epoch 3/150\n",
            "21/21 [==============================] - 65s 3s/step - loss: 0.0176 - acc: 0.9955 - val_loss: 0.6386 - val_acc: 0.8273\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.60972\n",
            "Learning Rate:  0.001157499966211617\n",
            "Epoch 4/150\n",
            "20/21 [===========================>..] - ETA: 3s - loss: 0.0222 - acc: 0.9938Learning Rate:  0.001157499966211617\n",
            "Epoch 4/150\n",
            "21/21 [==============================] - 66s 3s/step - loss: 0.0214 - acc: 0.9940 - val_loss: 0.6009 - val_acc: 0.8591\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.60972 to 0.60093, saving model to /content/gdrive/My Drive/vgg19_32:004-validation.hdf5\n",
            "Learning Rate:  0.0012100000167265534\n",
            "Epoch 5/150\n",
            "21/21 [==============================] - 65s 3s/step - loss: 0.0144 - acc: 0.9954 - val_loss: 0.6172 - val_acc: 0.8455\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.001262499950826168\n",
            "Epoch 6/150\n",
            "21/21 [==============================] - 65s 3s/step - loss: 0.0144 - acc: 0.9955 - val_loss: 0.6639 - val_acc: 0.8318\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.0013150000013411045\n",
            "Epoch 7/150\n",
            "21/21 [==============================] - 65s 3s/step - loss: 0.0154 - acc: 0.9940 - val_loss: 0.6401 - val_acc: 0.8545\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.001367500051856041\n",
            "Epoch 8/150\n",
            "20/21 [===========================>..] - ETA: 3s - loss: 0.0170 - acc: 0.9953\n",
            "Epoch 00007: val_loss did not improve from 0.60093\n",
            "21/21 [==============================] - 66s 3s/step - loss: 0.0164 - acc: 0.9955 - val_loss: 0.8122 - val_acc: 0.8136\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.0014199999859556556\n",
            "Epoch 9/150\n",
            "20/21 [===========================>..] - ETA: 3s - loss: 0.0161 - acc: 0.9922Learning Rate:  0.0014199999859556556\n",
            "21/21 [==============================] - 65s 3s/step - loss: 0.0155 - acc: 0.9926 - val_loss: 0.7203 - val_acc: 0.8273\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.0004656453966163099\n",
            "Epoch 10/150\n",
            "20/21 [===========================>..] - ETA: 3s - loss: 0.0123 - acc: 0.9969Learning Rate:  0.0004656453966163099\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.60093\n",
            "21/21 [==============================] - 66s 3s/step - loss: 0.0118 - acc: 0.9970 - val_loss: 0.7011 - val_acc: 0.8227\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.0015249999705702066\n",
            "Saved the Model after Epoch:  10\n",
            "Epoch 11/150\n",
            "21/21 [==============================] - 65s 3s/step - loss: 0.0131 - acc: 0.9940 - val_loss: 0.7896 - val_acc: 0.8273\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.001577500021085143\n",
            "Epoch 12/150\n",
            "20/21 [===========================>..] - ETA: 3s - loss: 0.0076 - acc: 0.9969\n",
            "Epoch 00011: val_loss did not improve from 0.60093\n",
            "21/21 [==============================] - 67s 3s/step - loss: 0.0105 - acc: 0.9955 - val_loss: 0.8774 - val_acc: 0.8318\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.0016299999551847577\n",
            "Epoch 13/150\n",
            "20/21 [===========================>..] - ETA: 3s - loss: 0.0129 - acc: 0.9953\n",
            "21/21 [==============================] - 65s 3s/step - loss: 0.0135 - acc: 0.9955 - val_loss: 0.7143 - val_acc: 0.8318\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.0016825000056996942\n",
            "Epoch 14/150\n",
            "20/21 [===========================>..] - ETA: 3s - loss: 0.0093 - acc: 0.9969Epoch 14/150\n",
            "21/21 [==============================] - 66s 3s/step - loss: 0.0089 - acc: 0.9970 - val_loss: 0.7859 - val_acc: 0.8500\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.000548655167222023\n",
            "Epoch 15/150\n",
            "20/21 [===========================>..] - ETA: 3s - loss: 0.0140 - acc: 0.9938\n",
            "Epoch 15/150\n",
            "21/21 [==============================] - 66s 3s/step - loss: 0.0165 - acc: 0.9909 - val_loss: 0.8179 - val_acc: 0.8318\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.0017874999903142452\n",
            "Epoch 16/150\n",
            "21/21 [==============================] - 66s 3s/step - loss: 0.0152 - acc: 0.9940 - val_loss: 0.7828 - val_acc: 0.8455\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.0018400000408291817\n",
            "Epoch 17/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py:610: UserWarning: The input 16 could not be retrieved. It could be because a worker has died.\n",
            "  UserWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "20/21 [===========================>..] - ETA: 3s - loss: 0.0075 - acc: 0.9953Learning Rate:  0.0018400000408291817\n",
            "Epoch 17/150\n",
            "21/21 [==============================] - 70s 3s/step - loss: 0.0084 - acc: 0.9955 - val_loss: 0.7778 - val_acc: 0.8318\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.60093\n",
            "Learning Rate:  0.0018924999749287963\n",
            "Epoch 18/150\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Process ForkPoolWorker-1070:\n",
            "Process ForkPoolWorker-1071:\n",
            "Process ForkPoolWorker-1072:\n",
            "Process ForkPoolWorker-1069:\n",
            "Traceback (most recent call last):\n",
            "Process ForkPoolWorker-1083:\n",
            "Process ForkPoolWorker-1085:\n",
            "Process ForkPoolWorker-1081:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Process ForkPoolWorker-1073:\n",
            "Process ForkPoolWorker-1082:\n",
            "Process ForkPoolWorker-1084:\n",
            "Process ForkPoolWorker-1074:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "Process ForkPoolWorker-1086:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
            "    res = self._reader.recv_bytes()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 108, in worker\n",
            "    task = get()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 334, in get\n",
            "    with self._rlock:\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
            "    buf = self._recv_bytes(maxlength)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
            "    buf = self._recv(4)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/synchronize.py\", line 95, in __enter__\n",
            "    return self._semlock.__enter__()\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Learning Rate:  0.0018924999749287963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
            "    chunk = read(handle, remaining)\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n",
            "KeyboardInterrupt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-ba497c77c6ff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# # Fit the model on the batches generated by datagen.flow().\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mvgg19_freezemodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_executed_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1656\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1658\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mbatch_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0msteps_done\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m                 \u001b[0mgenerator_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__len__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/utils/data_utils.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_running\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m                     \u001b[0mfuture\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m                     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqueue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtask_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/queue.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1bebHp1qoPj"
      },
      "source": [
        "vgg19_freezemodel.load_weights(\"/content/gdrive/My Drive/vgg19_ffreeze-validation.hdf5\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JTKwzEcwqoFf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "000d6e16-0dc1-4c0a-ae42-d3516d44ae72"
      },
      "source": [
        "scores=vgg16_freezemodel.evaluate_generator(validation_generator,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 17s 1s/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9J-fZ99qn9i",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bfb285c5-e774-4893-d2ac-e95eeb10a83b"
      },
      "source": [
        "#best scores after resnet 50 in zindi loss .40 is 0.50\n",
        "#scores  this is vgg16 score"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3809942907907746, 0.8681818181818182]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3bykaaIqnXt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCDUztsNQI83"
      },
      "source": [
        "#find optimal learning rate and fix this learning rate for further training\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlMQrFcT9dRs"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFHlaL5VSrAT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6996af8d-55c0-4df2-89c0-007939a754a1"
      },
      "source": [
        "test_generator.reset()\n",
        "a_predictions=vgg16_freezemodel.predict_generator(test_generator,verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "39/39 [==============================] - 48s 1s/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xkP2YZVTIJ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "32472d2e-4127-4a49-fcc3-49a6ac734ff8"
      },
      "source": [
        "a_data=a_predictions.round(decimals=3)\n",
        "a_data[0:6]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.   , 0.992, 0.008],\n",
              "       [0.   , 0.491, 0.509],\n",
              "       [0.   , 0.987, 0.013],\n",
              "       [0.955, 0.035, 0.01 ],\n",
              "       [0.993, 0.005, 0.002],\n",
              "       [0.   , 0.   , 1.   ]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1_yi41MTICr"
      },
      "source": [
        "predicted_class_indices=np.argmax(a_predictions,axis=1)\n",
        "labels = (train_generator.class_indices)\n",
        "labels1 = dict((v,k) for k,v in labels.items())\n",
        "pred = [labels1[k] for k in predicted_class_indices]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSwX1KreTHwd"
      },
      "source": [
        "filenames=test_generator.filenames\n",
        "results=pd.DataFrame({\"Filename\":filenames,\n",
        "                      \"Predictions\":pred})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ragMCTEGTilQ"
      },
      "source": [
        "def get_ids(filenames):\n",
        "  for i in filenames:\n",
        "    k=i.split('/')\n",
        "    kk=k[1].split('.')\n",
        "    return kk[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bs8t_ehBTnEZ"
      },
      "source": [
        "results['Filename']=results.apply(lambda x: get_ids(x),axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJsrMHEfTtFv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5a8f645d-59e6-44d3-e41d-fc89ba25dc1e"
      },
      "source": [
        "leaf_rust=pd.Series(range(610),name='leaf_rust',dtype=np.float32)\n",
        "stem_rust=pd.Series(range(610),name='stem_rust',dtype=np.float32)\n",
        "healthy_wheat=pd.Series(range(610),name='healthy_wheat',dtype=np.float32)\n",
        "sub=pd.concat([healthy_wheat,leaf_rust,stem_rust],axis=1)\n",
        "sub.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(610, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgVZE6BkT6LT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvuawNEMT6uk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34bb2689-f0af-4b64-99c1-7abfdf97bb2e"
      },
      "source": [
        "for i in tqdm(range(0,len(data))):\n",
        "  sub.loc[i]=data[i]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 610/610 [00:00<00:00, 5036.73it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCKtOcYxT_6c"
      },
      "source": [
        "sub[\"ID\"]=results['Filename']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EytIad3zUGFW"
      },
      "source": [
        "submit=sub[['ID','leaf_rust','stem_rust','healthy_wheat']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrzlouhN43RP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "outputId": "4177959e-32de-4861-c875-1e535380f2a6"
      },
      "source": [
        "submit.head(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>leaf_rust</th>\n",
              "      <th>stem_rust</th>\n",
              "      <th>healthy_wheat</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>008FWT</td>\n",
              "      <td>0.965</td>\n",
              "      <td>0.035</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>00AQXY</td>\n",
              "      <td>0.153</td>\n",
              "      <td>0.846</td>\n",
              "      <td>0.001</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>01OJZX</td>\n",
              "      <td>0.966</td>\n",
              "      <td>0.034</td>\n",
              "      <td>0.000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       ID  leaf_rust  stem_rust  healthy_wheat\n",
              "0  008FWT      0.965      0.035          0.000\n",
              "1  00AQXY      0.153      0.846          0.001\n",
              "2  01OJZX      0.966      0.034          0.000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vz2cYDC1UMmE"
      },
      "source": [
        "submit.to_csv('vgg161.csv',index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bJDngyL415L"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}